{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deit\n",
    "\n",
    "> Adapted from SCM/models/deit.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp models.deit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from functools import partial\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.functional import einsum\n",
    "from einops import rearrange, repeat\n",
    "\n",
    "from timm.models.registry import register_model\n",
    "from timm.models import create_model\n",
    "from timm.models import ResNet, Bottleneck\n",
    "from timm.models.layers.classifier import create_classifier\n",
    "from scm_nbdev.models.graph_fusion import Fuse\n",
    "from scm_nbdev.models.vision_transformer import VisionTransformer, _cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def embeddings_to_cosine_similarity_matrix(tokens):\n",
    "    \"\"\"\n",
    "    Shapes for inputs:\n",
    "    - tokens : :math:`(B, N, D)` where B is the batch size, N is the target `spatial` sequence length, D is the token representation length.\n",
    "\n",
    "    Shapes for outputs:\n",
    "\n",
    "    Converts a a tensor of D embeddings to an (N, N) tensor of similarities.\n",
    "    \"\"\"\n",
    "    dot = torch.einsum('bij, bkj -> bik', [tokens, tokens])\n",
    "    norm = torch.norm(tokens, p=2, dim=-1)\n",
    "    x = torch.div(dot, torch.einsum('bi, bj -> bij', norm, norm))\n",
    "\n",
    "    return x\n",
    "\n",
    "class SCM(VisionTransformer):\n",
    "    def __init__(self, num_layers=4, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        # self.head = nn.Linear(self.embed_dim, self.num_classes)\n",
    "        self.head = nn.Conv2d(self.embed_dim, self.num_classes,\n",
    "                              kernel_size=3, stride=1, padding=1)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.head.apply(self._init_weights)\n",
    "        self.num_layers = num_layers\n",
    "        self.layers = nn.ModuleList()\n",
    "        encoder = Encoder(dim=self.num_classes)\n",
    "        self.layers.append(encoder)\n",
    "        # self.convs = ConvModules(num_layers+1, self.num_classes)\n",
    "        for i in range(1, self.num_layers): \n",
    "            self.layers.append(Encoder(dim=self.num_classes, \n",
    "                                       fusion_cfg=dict(lapMat=encoder.fuse.laplacian, \n",
    "                                                 loss_rate=1, \n",
    "                                                 grid_size=(14, 14), \n",
    "                                                 iteration=4)))\n",
    "\n",
    "    def forward_features(self, x):\n",
    "        # taken from https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/vision_transformer.py\n",
    "        # with slight modifications to return patch embedding outputs\n",
    "        B = x.shape[0]\n",
    "        x = self.patch_embed(x)\n",
    "\n",
    "        # stole cls_tokens impl from Phil Wang, thanks\n",
    "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "        x = x + self.pos_embed\n",
    "        x = self.pos_drop(x)\n",
    "        attn_weights = []\n",
    "        for blk in self.blocks:\n",
    "            x, weights = blk(x)\n",
    "            attn_weights.append(weights)\n",
    "\n",
    "        x = self.norm(x)\n",
    "        return x[:, 0], x[:, 1:], attn_weights\n",
    "\n",
    "    def forward(self, x, test_select=0):\n",
    "        x_cls, x_patch, attn = self.forward_features(x)\n",
    "        n, p, c = x_patch.shape\n",
    "\n",
    "        x_patch = torch.reshape(x_patch, [n, int(p**0.5), int(p**0.5), c])\n",
    "        x_patch = x_patch.permute([0, 3, 1, 2])\n",
    "        x_patch = x_patch.contiguous()\n",
    "        x_patch = self.head(x_patch)\n",
    "\n",
    "        attn = torch.stack(attn)        # 12 * B * H * N * N\n",
    "        attn = torch.mean(attn, dim=2)  # 12 * B * N * N\n",
    "        \n",
    "        n, c, h, w = x_patch.shape\n",
    "        cams = attn.sum(0)[:, 0, 1:].reshape([n, h, w]).unsqueeze(1)\n",
    "        cam = rearrange(cams, 'B 1 H W -> B (H W)')\n",
    "        cam = norm_cam(cam)\n",
    "        pred_cam = rearrange(cam, 'B (H W) -> B 1 H W', H=h)\n",
    "        pred_semantic = x_patch\n",
    "        \n",
    "        if self.training:\n",
    "            for i, layer in enumerate(self.layers):\n",
    "                x_patch, cam = layer(x_patch, cam)\n",
    "            x_logits = self.avgpool(x_patch).squeeze(3).squeeze(2)\n",
    "            return x_logits\n",
    "        else:\n",
    "            x_logits = self.avgpool(pred_semantic).squeeze(3).squeeze(2)\n",
    "            predict = pred_cam*pred_semantic\n",
    "            if test_select!=0 and test_select>0:\n",
    "                topk_ind = torch.topk(x_logits, test_select)[-1]\n",
    "                predict = torch.tensor([torch.take(a, idx, axis=0) for (a, idx)  \n",
    "                                        in zip(cams, topk_ind)])\n",
    "            return x_logits, predict\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self,\n",
    "                 dim,\n",
    "                 thred=0.5,\n",
    "                 residual=True,\n",
    "                 fusion_cfg=dict(loss_rate=1, grid_size=(14, 14), iteration=16),\n",
    "                 ) -> None:\n",
    "        super().__init__()\n",
    "        self.fuse = Fuse(**fusion_cfg)\n",
    "        self.shrink = nn.Tanhshrink()\n",
    "        self.thred = nn.Parameter(torch.ones([1])*thred)\n",
    "        self.residual = residual\n",
    "        H, W = fusion_cfg['grid_size']\n",
    "        # self.norm = nn.LayerNorm((dim, H, W)) \n",
    "\n",
    "    def forward(self, x, cam):\n",
    "        \"\"\"foward function given x and spt\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): patch tokens, tensor of shape [B D H W]\n",
    "            cam (torch.Tensor): cam values, tensor of shape [B N]\n",
    "        Returns:\n",
    "            x (torch.Tensor): patch tokens, tensor of shape [B D H W]\n",
    "            cam (torch.Tensor): cam values, tensor of shape [B N]\n",
    "        \"\"\"\n",
    "        sim = embeddings_to_cosine_similarity_matrix(\n",
    "            rearrange(x, 'B D H W -> B (H W) D'))\n",
    "        thred = self.thred.to(x.device)\n",
    "        out_cam = einsum('b h w, b w -> b h', self.fuse(sim),cam)\n",
    "        thred = thred * cam.max(1, keepdim=True)[0]\n",
    "        out_cam = self.shrink(out_cam/thred)\n",
    "        out_cam = norm_cam(out_cam)\n",
    "        out_x = einsum('b d h w, b h w -> b d h w', x,\n",
    "                       rearrange(out_cam, 'B (H W) -> B H W', H=x.shape[-2]))\n",
    "        if self.residual:\n",
    "            x = x + out_x\n",
    "            cam = cam + out_cam\n",
    "        return x, cam\n",
    "\n",
    "\n",
    "def norm_cam(cam):\n",
    "    # cam [B N]\n",
    "    if len(cam.shape) == 3:\n",
    "        cam = cam - repeat(rearrange(cam, 'B H W -> B (H W)').min(1,\n",
    "                           keepdim=True)[0], 'B 1 -> B 1 1')\n",
    "        cam = cam / repeat(rearrange(cam, 'B H W -> B (H W)').max(1,\n",
    "                           keepdim=True)[0], 'B 1 -> B 1 1')\n",
    "    elif len(cam.shape) == 2:\n",
    "        cam = cam - cam.min(1, keepdim=True)[0]\n",
    "        cam = cam / cam.max(1, keepdim=True)[0]\n",
    "    elif len(cam.shape) == 4:\n",
    "        # min-max norm for each class feature map\n",
    "        B, C, H, W = cam.shape\n",
    "        cam = rearrange(cam, 'B C H W -> (B C) (H W)')\n",
    "        cam -= cam.min(1, keepdim=True)[0]\n",
    "        cam /= cam.max(1, keepdim=True)[0]\n",
    "        cam = rearrange(cam, '(B C) (H W) -> B C H W', B = B, H=H)\n",
    "    return cam\n",
    "\n",
    "\n",
    "class TSCAM(VisionTransformer):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.head = nn.Conv2d(self.embed_dim, self.num_classes,\n",
    "                              kernel_size=3, stride=1, padding=1)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "\n",
    "        self.head.apply(self._init_weights)\n",
    "\n",
    "    def forward_features(self, x):\n",
    "        # taken from https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/vision_transformer.py\n",
    "        # with slight modifications to return patch embedding outputs\n",
    "        B = x.shape[0]\n",
    "        x = self.patch_embed(x)\n",
    "\n",
    "        # stole cls_tokens impl from Phil Wang, thanks\n",
    "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "        x = x + self.pos_embed\n",
    "        x = self.pos_drop(x)\n",
    "        attn_weights = []\n",
    "        for blk in self.blocks:\n",
    "            x, weights = blk(x)\n",
    "            attn_weights.append(weights)\n",
    "\n",
    "        x = self.norm(x)\n",
    "        return x[:, 0], x[:, 1:], attn_weights\n",
    "\n",
    "    def forward(self, x, return_cam=False):\n",
    "        x_cls, x_patch, attn_weights = self.forward_features(x)\n",
    "        n, p, c = x_patch.shape\n",
    "        x_patch = torch.reshape(x_patch, [n, int(p**0.5), int(p**0.5), c])\n",
    "        x_patch = x_patch.permute([0, 3, 1, 2])\n",
    "        x_patch = x_patch.contiguous()\n",
    "        x_patch = self.head(x_patch)\n",
    "        x_logits = self.avgpool(x_patch).squeeze(3).squeeze(2)\n",
    "\n",
    "        if self.training:\n",
    "            return x_logits\n",
    "        else:\n",
    "            attn_weights = torch.stack(\n",
    "                attn_weights)        # 12 * B * H * N * N\n",
    "            attn_weights = torch.mean(attn_weights, dim=2)  # 12 * B * N * N\n",
    "\n",
    "            feature_map = x_patch.detach().clone()    # B * C * 14 * 14\n",
    "            n, c, h, w = feature_map.shape\n",
    "            cams = attn_weights.sum(0)[:, 0, 1:].reshape(\n",
    "                [n, h, w]).unsqueeze(1)\n",
    "            cams = cams * feature_map                           # B * C * 14 * 14\n",
    "\n",
    "            return x_logits, cams\n",
    "\n",
    "\n",
    "@register_model\n",
    "def deit_tscam_tiny_patch16_224(pretrained=False, **kwargs):\n",
    "    model = TSCAM(\n",
    "        patch_size=16, embed_dim=192, depth=12, num_heads=3, mlp_ratio=4, qkv_bias=True,\n",
    "        norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
    "    model.default_cfg = _cfg()\n",
    "    if pretrained:\n",
    "        checkpoint = torch.hub.load_state_dict_from_url(\n",
    "            url=\"https://dl.fbaipublicfiles.com/deit/deit_tiny_patch16_224-a1311bcf.pth\",\n",
    "            map_location=\"cpu\", check_hash=True\n",
    "        )['model']\n",
    "        model_dict = model.state_dict()\n",
    "\n",
    "        for k in ['head.weight', 'head.bias', 'head_dist.weight', 'head_dist.bias']:\n",
    "            if k in checkpoint and checkpoint[k].shape != model_dict[k].shape:\n",
    "                print(f\"Removing key {k} from pretrained checkpoint\")\n",
    "                del checkpoint[k]\n",
    "\n",
    "        pretrained_dict = {k: v for k,\n",
    "                           v in checkpoint.items() if k in model_dict}\n",
    "        model_dict.update(pretrained_dict)\n",
    "        model.load_state_dict(model_dict)\n",
    "    return model\n",
    "\n",
    "\n",
    "@register_model\n",
    "def deit_tscam_small_patch16_224(pretrained=False, **kwargs):\n",
    "    model = TSCAM(\n",
    "        patch_size=16, embed_dim=384, depth=12, num_heads=6, mlp_ratio=4, qkv_bias=True,\n",
    "        norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
    "    model.default_cfg = _cfg()\n",
    "    if pretrained:\n",
    "        checkpoint = torch.hub.load_state_dict_from_url(\n",
    "            url=\"https://dl.fbaipublicfiles.com/deit/deit_small_patch16_224-cd65a155.pth\",\n",
    "            map_location=\"cpu\", check_hash=True\n",
    "        )['model']\n",
    "        model_dict = model.state_dict()\n",
    "        for k in ['head.weight', 'head.bias', 'head_dist.weight', 'head_dist.bias']:\n",
    "            if k in checkpoint and checkpoint[k].shape != model_dict[k].shape:\n",
    "                print(f\"Removing key {k} from pretrained checkpoint\")\n",
    "                del checkpoint[k]\n",
    "        pretrained_dict = {k: v for k,\n",
    "                           v in checkpoint.items() if k in model_dict}\n",
    "        model_dict.update(pretrained_dict)\n",
    "        model.load_state_dict(model_dict)\n",
    "    return model\n",
    "\n",
    "\n",
    "@register_model\n",
    "def deit_tscam_base_patch16_224(pretrained=False, **kwargs):\n",
    "    model = TSCAM(\n",
    "        patch_size=16, embed_dim=768, depth=12, num_heads=12, mlp_ratio=4, qkv_bias=True,\n",
    "        norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
    "    model.default_cfg = _cfg()\n",
    "    if pretrained:\n",
    "        checkpoint = torch.hub.load_state_dict_from_url(\n",
    "            url=\"https://dl.fbaipublicfiles.com/deit/deit_base_patch16_224-b5f2ef4d.pth\",\n",
    "            map_location=\"cpu\", check_hash=True\n",
    "        )['model']\n",
    "        model_dict = model.state_dict()\n",
    "        for k in ['head.weight', 'head.bias', 'head_dist.weight', 'head_dist.bias']:\n",
    "            if k in checkpoint and checkpoint[k].shape != model_dict[k].shape:\n",
    "                print(f\"Removing key {k} from pretrained checkpoint\")\n",
    "                del checkpoint[k]\n",
    "        pretrained_dict = {k: v for k,\n",
    "                           v in checkpoint.items() if k in model_dict}\n",
    "        model_dict.update(pretrained_dict)\n",
    "        model.load_state_dict(model_dict)\n",
    "    return model\n",
    "\n",
    "\n",
    "@register_model\n",
    "def deit_scm_tiny_patch16_224(pretrained=False, **kwargs):\n",
    "    model = SCM(\n",
    "        patch_size=16, embed_dim=192, depth=12, num_heads=3, mlp_ratio=4, qkv_bias=True,\n",
    "        norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
    "    model.default_cfg = _cfg()\n",
    "    if pretrained:\n",
    "        checkpoint = torch.hub.load_state_dict_from_url(\n",
    "            url=\"https://dl.fbaipublicfiles.com/deit/deit_tiny_patch16_224-a1311bcf.pth\",\n",
    "            map_location=\"cpu\", check_hash=True\n",
    "        )['model']\n",
    "        model_dict = model.state_dict()\n",
    "\n",
    "        for k in ['head.weight', 'head.bias', 'head_dist.weight', 'head_dist.bias']:\n",
    "            if k in checkpoint and checkpoint[k].shape != model_dict[k].shape:\n",
    "                print(f\"Removing key {k} from pretrained checkpoint\")\n",
    "                del checkpoint[k]\n",
    "\n",
    "        pretrained_dict = {k: v for k,\n",
    "                           v in checkpoint.items() if k in model_dict}\n",
    "        model_dict.update(pretrained_dict)\n",
    "        model.load_state_dict(model_dict)\n",
    "    return model\n",
    "\n",
    "\n",
    "@register_model\n",
    "def deit_scm_small_patch16_224(pretrained=False, **kwargs):\n",
    "    model = SCM(\n",
    "        patch_size=16, embed_dim=384, depth=12, num_heads=6, mlp_ratio=4, qkv_bias=True,\n",
    "        norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
    "    model.default_cfg = _cfg()\n",
    "    if pretrained:\n",
    "        checkpoint = torch.hub.load_state_dict_from_url(\n",
    "            url=\"https://dl.fbaipublicfiles.com/deit/deit_small_patch16_224-cd65a155.pth\",\n",
    "            map_location=\"cpu\", check_hash=True\n",
    "        )['model']\n",
    "        model_dict = model.state_dict()\n",
    "        for k in ['head.weight', 'head.bias', 'head_dist.weight', 'head_dist.bias']:\n",
    "            if k in checkpoint and checkpoint[k].shape != model_dict[k].shape:\n",
    "                print(f\"Removing key {k} from pretrained checkpoint\")\n",
    "                del checkpoint[k]\n",
    "        pretrained_dict = {k: v for k,\n",
    "                           v in checkpoint.items() if k in model_dict}\n",
    "        model_dict.update(pretrained_dict)\n",
    "        model.load_state_dict(model_dict)\n",
    "    return model\n",
    "\n",
    "@register_model\n",
    "def vit_scm_small_patch16_224(pretrained=False, **kwargs):\n",
    "    model = SCM(\n",
    "        patch_size=16, embed_dim=768, depth=8, num_heads=8, mlp_ratio=3., qkv_bias=True, qk_scale=768 ** -0.5,\n",
    "        **kwargs)\n",
    "    model.default_cfg = _cfg()\n",
    "    if pretrained:\n",
    "        # checkpoint = torch.hub.load_state_dict_from_url(\n",
    "        #     url=\"https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/vit_small_p16_224-15ec54c9.pth\",\n",
    "        #     map_location=\"cpu\", check_hash=True\n",
    "        # )\n",
    "        pre_vit_small = create_model('vit_small_patch16_224', pretrained=True)\n",
    "        checkpoint = pre_vit_small.state_dict()\n",
    "\n",
    "        model_dict = model.state_dict()\n",
    "        for k in ['head.weight', 'head.bias']:\n",
    "            if k in checkpoint and checkpoint[k].shape != model_dict[k].shape:\n",
    "                print(f\"Removing key {k} from pretrained checkpoint\")\n",
    "                del checkpoint[k]\n",
    "        pretrained_dict = {k: v for k,\n",
    "                           v in checkpoint.items() if k in model_dict}\n",
    "        model_dict.update(pretrained_dict)\n",
    "        model.load_state_dict(model_dict)\n",
    "    return model\n",
    "\n",
    "@register_model\n",
    "def deit_scm_base_patch16_224(pretrained=False, **kwargs):\n",
    "    model = SCM(\n",
    "        patch_size=16, embed_dim=768, depth=12, num_heads=12, mlp_ratio=4, qkv_bias=True,\n",
    "        norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
    "    model.default_cfg = _cfg()\n",
    "    if pretrained:\n",
    "        checkpoint = torch.hub.load_state_dict_from_url(\n",
    "            url=\"https://dl.fbaipublicfiles.com/deit/deit_base_patch16_224-b5f2ef4d.pth\",\n",
    "            map_location=\"cpu\", check_hash=True\n",
    "        )['model']\n",
    "        model_dict = model.state_dict()\n",
    "        for k in ['head.weight', 'head.bias', 'head_dist.weight', 'head_dist.bias']:\n",
    "            if k in checkpoint and checkpoint[k].shape != model_dict[k].shape:\n",
    "                print(f\"Removing key {k} from pretrained checkpoint\")\n",
    "                del checkpoint[k]\n",
    "        pretrained_dict = {k: v for k,\n",
    "                           v in checkpoint.items() if k in model_dict}\n",
    "        model_dict.update(pretrained_dict)\n",
    "        model.load_state_dict(model_dict)\n",
    "    return model\n",
    "\n",
    "@register_model\n",
    "def deit_scm_small_patch16_384(pretrained=False, **kwargs):\n",
    "    kwargs['img_size']=384\n",
    "    model = SCM(\n",
    "        patch_size=16, embed_dim=384, depth=12, num_heads=6, mlp_ratio=4, qkv_bias=True,\n",
    "        norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
    "    model.default_cfg = _cfg()\n",
    "    return model\n",
    "\n",
    "@register_model\n",
    "def deit_scm_base_patch16_384(pretrained=False, **kwargs):\n",
    "    kwargs['img_size']=384\n",
    "    model = SCM(\n",
    "        patch_size=16, embed_dim=768, depth=12, num_heads=12, mlp_ratio=4, qkv_bias=True,\n",
    "        norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
    "    model.default_cfg = _cfg()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SCM",
   "language": "python",
   "name": "scm"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

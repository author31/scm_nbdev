{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Engine\n",
    "\n",
    "> Adapted from SCM/lib/engine.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp core.engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import os\n",
    "import shutil\n",
    "import torch\n",
    "import numpy as np\n",
    "from loguru import logger\n",
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "from scm_nbdev.datasets.cub import CUBDataset\n",
    "from scm_nbdev.datasets.imagenet import ImageNetDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def create_data_loader(cfg, root_dir):\n",
    "    \"\"\" Create data_loaders for training and validation\n",
    "    :param cfg: hyperparameter configuration\n",
    "    :param root_dir: dataset root path\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    logger.info('preparing data...')\n",
    "    if cfg.DATA.DATASET == 'CUB':\n",
    "        train_loader = torch.utils.data.DataLoader(\n",
    "            CUBDataset(root=root_dir, cfg=cfg, is_train=True),\n",
    "            batch_size=cfg.TRAIN.BATCH_SIZE, shuffle=True, num_workers=cfg.BASIC.NUM_WORKERS, pin_memory=True)\n",
    "        test_loader = torch.utils.data.DataLoader(\n",
    "            CUBDataset(root=root_dir, cfg=cfg, is_train=False),\n",
    "            batch_size=cfg.TEST.BATCH_SIZE, shuffle=False, num_workers=cfg.BASIC.NUM_WORKERS, pin_memory=True)\n",
    "        # val_loader = torch.utils.data.DataLoader(\n",
    "        #     CUBDataset(root=root_dir, cfg=cfg, is_train=False, val=True),\n",
    "        #     batch_size=cfg.TEST.BATCH_SIZE, shuffle=False, num_workers=cfg.BASIC.NUM_WORKERS, pin_memory=True)\n",
    "        val_loader = None\n",
    "    elif cfg.DATA.DATASET == 'ImageNet':\n",
    "        \n",
    "        train_loader = torch.utils.data.DataLoader(\n",
    "            ImageNetDataset(root=root_dir, cfg=cfg, is_train=True),\n",
    "            batch_size=cfg.TRAIN.BATCH_SIZE, shuffle=True, num_workers=cfg.BASIC.NUM_WORKERS, pin_memory=True)\n",
    "        test_loader = torch.utils.data.DataLoader(\n",
    "            ImageNetDataset(root=root_dir, cfg=cfg, is_train=False),\n",
    "            batch_size=cfg.TEST.BATCH_SIZE, shuffle=False, num_workers=cfg.BASIC.NUM_WORKERS, pin_memory=True)\n",
    "        val_loader = torch.utils.data.DataLoader(\n",
    "            ImageNetDataset(root=root_dir, cfg=cfg, is_train=False, val=True),\n",
    "            batch_size=cfg.TEST.BATCH_SIZE, shuffle=False, num_workers=cfg.BASIC.NUM_WORKERS, pin_memory=True)\n",
    "                \n",
    "    logger.info('done loading..')\n",
    "    return train_loader, test_loader, val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def str_gpus(ids):\n",
    "    str_ids = ''\n",
    "    for i in ids:\n",
    "        str_ids =  str_ids + str(i)\n",
    "        str_ids =  str_ids + ','\n",
    "    return str_ids\n",
    "\n",
    "def map_sklearn(labels, results):\n",
    "    _map = average_precision_score(labels, results, average=\"micro\")\n",
    "    return _map\n",
    "\n",
    "\n",
    "#must be moved to external lr_scheduler\n",
    "def adjust_learning_rate(optimizer, epoch, cfg):\n",
    "    \"\"\"\"Sets the learning rate to the initial LR decayed by lr_factor\"\"\"\n",
    "    lr_decay = cfg.SOLVER.LR_FACTOR**(sum(epoch > np.array(cfg.SOLVER.LR_STEPS)))\n",
    "    lr = cfg.SOLVER.START_LR * lr_decay\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr * param_group['lr_mult']\n",
    "\n",
    "\n",
    "def adjust_lr_by_scheduler(lr_scheduler, optimizer, curr_iter):\n",
    "    \"\"\" Adjust the learning rate of model parameters by scheduler\n",
    "    :param optimizer: optimizer (e.g. SGD, AdamW, Adam)\n",
    "    :param epoch: training epoch\n",
    "    :param curr_iter: current iteration\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    lr = lr_scheduler.update_lr(curr_iter)\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group[\"lr\"] = lr\n",
    "\n",
    "def save_checkpoint(state, save_dir, epoch, is_best):\n",
    "    filename = os.path.join(save_dir, 'ckpt_'+str(epoch)+'.pth.tar')\n",
    "    torch.save(state, filename)\n",
    "    if is_best:\n",
    "        best_name = os.path.join(save_dir, 'model_best.pth.tar')\n",
    "        shutil.copyfile(filename, best_name)\n",
    "\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "#maybe moved to external metrics module\n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\" Computes the precision@k for the specified values of k\n",
    "    :param output: tensor of shape B x K, predicted logits of image from model\n",
    "    :param target: tensor of shape B X 1, ground-truth logits of image\n",
    "    :param topk: top predictions\n",
    "    :return: list of precision@k\n",
    "    \"\"\"\n",
    "    maxk = max(topk)\n",
    "    batch_size = target.size(0)\n",
    "\n",
    "    _, pred = output.topk(1, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].reshape(-1).float().sum(0)\n",
    "        res.append(correct_k.mul_(100.0 / batch_size))\n",
    "    return res\n",
    "\n",
    "\n",
    "def list2acc(results_list):\n",
    "    \"\"\"\n",
    "    :param results_list: list contains 0 and 1\n",
    "    :return: accuarcy\n",
    "    \"\"\"\n",
    "    accuarcy = results_list.count(1)/len(results_list)\n",
    "    return accuarcy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SCM",
   "language": "python",
   "name": "scm"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conformer\n",
    "\n",
    "> Adpated from SCM/models/conformer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp models.conformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "# -----------------------------------------------------------------------------------------------------\n",
    "# TS-CAM using Conformer as backbone is implemented based on https://github.com/pengzhiliang/Conformer\n",
    "# -----------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from functools import partial\n",
    "\n",
    "from timm.models.layers import DropPath, trunc_normal_\n",
    "from timm.models.registry import register_model\n",
    "\n",
    "from scm_nbdev.models.graph_fusion import Fuse\n",
    "from einops import rearrange, repeat\n",
    "from torch.functional import einsum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Mlp(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.act = act_layer()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0., vis=False):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        # NOTE scale factor was wrong in my original version, can set manually to be compat with prev weights\n",
    "        self.scale = qk_scale or head_dim ** -0.5\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "        self.vis = vis\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, N, C = x.shape\n",
    "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]  # make torchscript happy (cannot use tensor as tuple)\n",
    "\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "\n",
    "        #if self.vis:\n",
    "        #    print(\"return attention map for visulization\")\n",
    "        #    return x, attn\n",
    "        #return x, None\n",
    "        return x, attn\n",
    "\n",
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n",
    "                 drop_path=0., act_layer=nn.GELU, norm_layer=partial(nn.LayerNorm, eps=1e-6), vis=False):\n",
    "        super().__init__()\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.attn = Attention(\n",
    "            dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop, vis=vis)\n",
    "        # NOTE: drop path for stochastic depth, we shall see if this is better than dropout here\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y, attn_weight = self.attn(self.norm1(x))\n",
    "        x = x + self.drop_path(y)\n",
    "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
    "        return x, attn_weight\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, inplanes, outplanes, stride=1, res_conv=False, act_layer=nn.ReLU, groups=1,\n",
    "                 norm_layer=partial(nn.BatchNorm2d, eps=1e-6), drop_block=None, drop_path=None):\n",
    "        super(ConvBlock, self).__init__()\n",
    "\n",
    "        expansion = 4\n",
    "        med_planes = outplanes // expansion\n",
    "\n",
    "        self.conv1 = nn.Conv2d(inplanes, med_planes, kernel_size=1, stride=1, padding=0, bias=False)\n",
    "        self.bn1 = norm_layer(med_planes)\n",
    "        self.act1 = act_layer(inplace=True)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(med_planes, med_planes, kernel_size=3, stride=stride, groups=groups, padding=1, bias=False)\n",
    "        self.bn2 = norm_layer(med_planes)\n",
    "        self.act2 = act_layer(inplace=True)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(med_planes, outplanes, kernel_size=1, stride=1, padding=0, bias=False)\n",
    "        self.bn3 = norm_layer(outplanes)\n",
    "        self.act3 = act_layer(inplace=True)\n",
    "\n",
    "        if res_conv:\n",
    "            self.residual_conv = nn.Conv2d(inplanes, outplanes, kernel_size=1, stride=stride, padding=0, bias=False)\n",
    "            self.residual_bn = norm_layer(outplanes)\n",
    "\n",
    "        self.res_conv = res_conv\n",
    "        self.drop_block = drop_block\n",
    "        self.drop_path = drop_path\n",
    "\n",
    "    def zero_init_last_bn(self):\n",
    "        nn.init.zeros_(self.bn3.weight)\n",
    "\n",
    "    def forward(self, x, x_t=None, return_x_2=True):\n",
    "        residual = x\n",
    "\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        if self.drop_block is not None:\n",
    "            x = self.drop_block(x)\n",
    "        x = self.act1(x)\n",
    "\n",
    "        x = self.conv2(x) if x_t is None else self.conv2(x + x_t)\n",
    "        x = self.bn2(x)\n",
    "        if self.drop_block is not None:\n",
    "            x = self.drop_block(x)\n",
    "        x2 = self.act2(x)\n",
    "\n",
    "        x = self.conv3(x2)\n",
    "        x = self.bn3(x)\n",
    "        if self.drop_block is not None:\n",
    "            x = self.drop_block(x)\n",
    "\n",
    "        if self.drop_path is not None:\n",
    "            x = self.drop_path(x)\n",
    "\n",
    "        if self.res_conv:\n",
    "            residual = self.residual_conv(residual)\n",
    "            residual = self.residual_bn(residual)\n",
    "\n",
    "        x += residual\n",
    "        x = self.act3(x)\n",
    "\n",
    "        if return_x_2:\n",
    "            return x, x2\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "\n",
    "class FCUDown(nn.Module):\n",
    "    \"\"\" CNN feature maps -> Transformer patch embeddings\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, inplanes, outplanes, dw_stride, act_layer=nn.GELU,\n",
    "                 norm_layer=partial(nn.LayerNorm, eps=1e-6)):\n",
    "        super(FCUDown, self).__init__()\n",
    "        self.dw_stride = dw_stride\n",
    "\n",
    "        self.conv_project = nn.Conv2d(inplanes, outplanes, kernel_size=1, stride=1, padding=0)\n",
    "        self.sample_pooling = nn.AvgPool2d(kernel_size=dw_stride, stride=dw_stride)\n",
    "\n",
    "        self.ln = norm_layer(outplanes)\n",
    "        self.act = act_layer()\n",
    "\n",
    "    def forward(self, x, x_t):\n",
    "        x = self.conv_project(x)  # [N, C, H, W]\n",
    "\n",
    "        x = self.sample_pooling(x).flatten(2).transpose(1, 2)\n",
    "        x = self.ln(x)\n",
    "        x = self.act(x)\n",
    "\n",
    "        x = torch.cat([x_t[:, 0][:, None, :], x], dim=1)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class FCUUp(nn.Module):\n",
    "    \"\"\" Transformer patch embeddings -> CNN feature maps\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, inplanes, outplanes, up_stride, act_layer=nn.ReLU,\n",
    "                 norm_layer=partial(nn.BatchNorm2d, eps=1e-6),):\n",
    "        super(FCUUp, self).__init__()\n",
    "\n",
    "        self.up_stride = up_stride\n",
    "        self.conv_project = nn.Conv2d(inplanes, outplanes, kernel_size=1, stride=1, padding=0)\n",
    "        self.bn = norm_layer(outplanes)\n",
    "        self.act = act_layer()\n",
    "\n",
    "    def forward(self, x, H, W):\n",
    "        B, _, C = x.shape\n",
    "        # [N, 197, 384] -> [N, 196, 384] -> [N, 384, 196] -> [N, 384, 14, 14]\n",
    "        x_r = x[:, 1:].transpose(1, 2).reshape(B, C, H, W)\n",
    "        x_r = self.act(self.bn(self.conv_project(x_r)))\n",
    "\n",
    "        return F.interpolate(x_r, size=(H * self.up_stride, W * self.up_stride))\n",
    "\n",
    "\n",
    "class Med_ConvBlock(nn.Module):\n",
    "    \"\"\" special case for Convblock with down sampling,\n",
    "    \"\"\"\n",
    "    def __init__(self, inplanes, act_layer=nn.ReLU, groups=1, norm_layer=partial(nn.BatchNorm2d, eps=1e-6),\n",
    "                 drop_block=None, drop_path=None):\n",
    "\n",
    "        super(Med_ConvBlock, self).__init__()\n",
    "\n",
    "        expansion = 4\n",
    "        med_planes = inplanes // expansion\n",
    "\n",
    "        self.conv1 = nn.Conv2d(inplanes, med_planes, kernel_size=1, stride=1, padding=0, bias=False)\n",
    "        self.bn1 = norm_layer(med_planes)\n",
    "        self.act1 = act_layer(inplace=True)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(med_planes, med_planes, kernel_size=3, stride=1, groups=groups, padding=1, bias=False)\n",
    "        self.bn2 = norm_layer(med_planes)\n",
    "        self.act2 = act_layer(inplace=True)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(med_planes, inplanes, kernel_size=1, stride=1, padding=0, bias=False)\n",
    "        self.bn3 = norm_layer(inplanes)\n",
    "        self.act3 = act_layer(inplace=True)\n",
    "\n",
    "        self.drop_block = drop_block\n",
    "        self.drop_path = drop_path\n",
    "\n",
    "    def zero_init_last_bn(self):\n",
    "        nn.init.zeros_(self.bn3.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        if self.drop_block is not None:\n",
    "            x = self.drop_block(x)\n",
    "        x = self.act1(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        if self.drop_block is not None:\n",
    "            x = self.drop_block(x)\n",
    "        x = self.act2(x)\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "        if self.drop_block is not None:\n",
    "            x = self.drop_block(x)\n",
    "\n",
    "        if self.drop_path is not None:\n",
    "            x = self.drop_path(x)\n",
    "\n",
    "        x += residual\n",
    "        x = self.act3(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class ConvTransBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Basic module for ConvTransformer, keep feature maps for CNN block and patch embeddings for transformer encoder block\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, inplanes, outplanes, res_conv, stride, dw_stride, embed_dim, num_heads=12, mlp_ratio=4.,\n",
    "                 qkv_bias=False, qk_scale=None, drop_rate=0., attn_drop_rate=0., drop_path_rate=0.,\n",
    "                 last_fusion=False, num_med_block=0, groups=1, vis=True):\n",
    "\n",
    "        super(ConvTransBlock, self).__init__()\n",
    "        expansion = 4\n",
    "        self.cnn_block = ConvBlock(inplanes=inplanes, outplanes=outplanes, res_conv=res_conv, stride=stride, groups=groups)\n",
    "\n",
    "        if last_fusion:\n",
    "            self.fusion_block = ConvBlock(inplanes=outplanes, outplanes=outplanes, stride=2, res_conv=True, groups=groups)\n",
    "        else:\n",
    "            self.fusion_block = ConvBlock(inplanes=outplanes, outplanes=outplanes, groups=groups)\n",
    "\n",
    "        if num_med_block > 0:\n",
    "            self.med_block = []\n",
    "            for i in range(num_med_block):\n",
    "                self.med_block.append(Med_ConvBlock(inplanes=outplanes, groups=groups))\n",
    "            self.med_block = nn.ModuleList(self.med_block)\n",
    "\n",
    "        self.squeeze_block = FCUDown(inplanes=outplanes // expansion, outplanes=embed_dim, dw_stride=dw_stride)\n",
    "\n",
    "        self.expand_block = FCUUp(inplanes=embed_dim, outplanes=outplanes // expansion, up_stride=dw_stride)\n",
    "\n",
    "        self.trans_block = Block(\n",
    "            dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
    "            drop=drop_rate, attn_drop=attn_drop_rate, drop_path=drop_path_rate, vis=vis)\n",
    "\n",
    "        self.dw_stride = dw_stride\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_med_block = num_med_block\n",
    "        self.last_fusion = last_fusion\n",
    "\n",
    "    def forward(self, x, x_t):\n",
    "        x, x2 = self.cnn_block(x)\n",
    "\n",
    "        _, _, H, W = x2.shape\n",
    "\n",
    "        x_st = self.squeeze_block(x2, x_t)\n",
    "\n",
    "        x_t, attn_weight = self.trans_block(x_st + x_t)\n",
    "\n",
    "        if self.num_med_block > 0:\n",
    "            for m in self.med_block:\n",
    "                x = m(x)\n",
    "\n",
    "        x_t_r = self.expand_block(x_t, H // self.dw_stride, W // self.dw_stride)\n",
    "        x = self.fusion_block(x, x_t_r, return_x_2=False)\n",
    "\n",
    "        return x, x_t, attn_weight\n",
    "\n",
    "\n",
    "class Conformer(nn.Module):\n",
    "\n",
    "    def __init__(self, patch_size=16, in_chans=3, num_classes=1000, base_channel=64, channel_ratio=4, num_med_block=0,\n",
    "                 embed_dim=768, depth=12, num_heads=12, mlp_ratio=4., qkv_bias=False, qk_scale=None, num_layers=4,\n",
    "                 drop_rate=0., attn_drop_rate=0., drop_path_rate=0.):\n",
    "\n",
    "        # Transformer\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.num_features = self.embed_dim = embed_dim  # num_features for consistency with other models\n",
    "        assert depth % 3 == 0\n",
    "\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        self.trans_dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule\n",
    "\n",
    "        # Classifier head\n",
    "        self.trans_norm = nn.LayerNorm(embed_dim)\n",
    "        self.trans_cls_head = nn.Conv2d(self.embed_dim, self.num_classes, kernel_size=3, stride=1, padding=1)\n",
    "        self.pooling = nn.AdaptiveAvgPool2d(1)\n",
    "        self.conv_cls_head = nn.Linear(int(256 * channel_ratio), num_classes)\n",
    "\n",
    "        # Stem stage: get the feature maps by conv block (copied form resnet.py)\n",
    "        self.conv1 = nn.Conv2d(in_chans, 64, kernel_size=7, stride=2, padding=3, bias=False)  # 1 / 2 [112, 112]\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.act1 = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)  # 1 / 4 [56, 56]\n",
    "\n",
    "        # 1 stage\n",
    "        stage_1_channel = int(base_channel * channel_ratio)\n",
    "        trans_dw_stride = patch_size // 4\n",
    "        self.conv_1 = ConvBlock(inplanes=64, outplanes=stage_1_channel, res_conv=True, stride=1)\n",
    "        self.trans_patch_conv = nn.Conv2d(64, embed_dim, kernel_size=trans_dw_stride, stride=trans_dw_stride, padding=0)\n",
    "        self.trans_1 = Block(dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias,\n",
    "                             qk_scale=qk_scale, drop=drop_rate, attn_drop=attn_drop_rate, drop_path=self.trans_dpr[0],\n",
    "                             )\n",
    "\n",
    "        # 2~4 stage\n",
    "        init_stage = 2\n",
    "        fin_stage = depth // 3 + 1\n",
    "        for i in range(init_stage, fin_stage):\n",
    "            self.add_module('conv_trans_' + str(i),\n",
    "                    ConvTransBlock(\n",
    "                        stage_1_channel, stage_1_channel, False, 1, dw_stride=trans_dw_stride, embed_dim=embed_dim,\n",
    "                        num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
    "                        drop_rate=drop_rate, attn_drop_rate=attn_drop_rate, drop_path_rate=self.trans_dpr[i-1],\n",
    "                        num_med_block=num_med_block\n",
    "                    )\n",
    "            )\n",
    "\n",
    "\n",
    "        stage_2_channel = int(base_channel * channel_ratio * 2)\n",
    "        # 5~8 stage\n",
    "        init_stage = fin_stage # 5\n",
    "        fin_stage = fin_stage + depth // 3 # 9\n",
    "        for i in range(init_stage, fin_stage):\n",
    "            s = 2 if i == init_stage else 1\n",
    "            in_channel = stage_1_channel if i == init_stage else stage_2_channel\n",
    "            res_conv = True if i == init_stage else False\n",
    "            self.add_module('conv_trans_' + str(i),\n",
    "                    ConvTransBlock(\n",
    "                        in_channel, stage_2_channel, res_conv, s, dw_stride=trans_dw_stride // 2, embed_dim=embed_dim,\n",
    "                        num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
    "                        drop_rate=drop_rate, attn_drop_rate=attn_drop_rate, drop_path_rate=self.trans_dpr[i-1],\n",
    "                        num_med_block=num_med_block\n",
    "                    )\n",
    "            )\n",
    "\n",
    "        stage_3_channel = int(base_channel * channel_ratio * 2 * 2)\n",
    "        # 9~12 stage\n",
    "        init_stage = fin_stage  # 9\n",
    "        fin_stage = fin_stage + depth // 3  # 13\n",
    "        for i in range(init_stage, fin_stage):\n",
    "            s = 2 if i == init_stage else 1\n",
    "            in_channel = stage_2_channel if i == init_stage else stage_3_channel\n",
    "            res_conv = True if i == init_stage else False\n",
    "            last_fusion = True if i == depth else False\n",
    "            self.add_module('conv_trans_' + str(i),\n",
    "                    ConvTransBlock(\n",
    "                        in_channel, stage_3_channel, res_conv, s, dw_stride=trans_dw_stride // 4, embed_dim=embed_dim,\n",
    "                        num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
    "                        drop_rate=drop_rate, attn_drop_rate=attn_drop_rate, drop_path_rate=self.trans_dpr[i-1],\n",
    "                        num_med_block=num_med_block, last_fusion=last_fusion\n",
    "                    )\n",
    "            )\n",
    "        self.fin_stage = fin_stage\n",
    "        \n",
    "        self.num_layers = num_layers\n",
    "        self.layers = nn.ModuleList()\n",
    "        for i in range(self.num_layers):\n",
    "            self.layers.append(Encoder())         \n",
    "\n",
    "        trunc_normal_(self.cls_token, std=.02)\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            trunc_normal_(m.weight, std=.02)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "        elif isinstance(m, nn.Conv2d):\n",
    "            nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "        elif isinstance(m, nn.BatchNorm2d):\n",
    "            nn.init.constant_(m.weight, 1.)\n",
    "            nn.init.constant_(m.bias, 0.)\n",
    "        elif isinstance(m, nn.GroupNorm):\n",
    "            nn.init.constant_(m.weight, 1.)\n",
    "            nn.init.constant_(m.bias, 0.)\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def no_weight_decay(self):\n",
    "        return {'cls_token'}\n",
    "\n",
    "\n",
    "    def forward(self, x, return_cam=False):\n",
    "        B = x.shape[0]\n",
    "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
    "\n",
    "        # pdb.set_trace()\n",
    "        # stem stage [N, 3, 224, 224] -> [N, 64, 56, 56]\n",
    "        x_base = self.maxpool(self.act1(self.bn1(self.conv1(x))))\n",
    "\n",
    "        attn_weights = []\n",
    "\n",
    "        # 1 stage\n",
    "        x = self.conv_1(x_base, return_x_2=False)\n",
    "\n",
    "        x_t = self.trans_patch_conv(x_base).flatten(2).transpose(1, 2)\n",
    "        x_t = torch.cat([cls_tokens, x_t], dim=1)\n",
    "        x_t, attn_weight = self.trans_1(x_t)\n",
    "        attn_weights.append(attn_weight)\n",
    "\n",
    "        # 2 ~ final\n",
    "        for i in range(2, self.fin_stage):\n",
    "            x, x_t, attn_weight = eval('self.conv_trans_' + str(i))(x, x_t)\n",
    "            attn_weights.append(attn_weight)\n",
    "\n",
    "        # conv classification\n",
    "        x_conv = self.pooling(x).flatten(1)\n",
    "        conv_cls = self.conv_cls_head(x_conv)\n",
    "\n",
    "        # trans classification\n",
    "        x_t = self.trans_norm(x_t)\n",
    "        x_patch = x_t[:, 1:]\n",
    "        n, p, c = x_patch.shape\n",
    "        x_patch = torch.reshape(x_patch, [n, int(p ** 0.5), int(p ** 0.5), c])\n",
    "        x_patch = x_patch.permute([0, 3, 1, 2]).contiguous()\n",
    "        x_patch = self.trans_cls_head(x_patch)\n",
    "        pred_box = x_patch\n",
    "\n",
    "        attn_weights = torch.stack(attn_weights)\n",
    "        attn_weights = torch.mean(attn_weights, dim=2)  # 12 * B * N * N\n",
    "        # residual_att = torch.eye(attn_weights.size(2)).unsqueeze(0).unsqueeze(1).to(attn_weights.get_device())    # 12 * B * N * N\n",
    "        # aug_att_mat = attn_weights + residual_att\n",
    "        # aug_att_mat = aug_att_mat / aug_att_mat.sum(dim=-1).unsqueeze(-1)\n",
    "\n",
    "        # joint_attns = torch.zeros(aug_att_mat.size()).to(attn_weights.get_device())\n",
    "        # joint_attns_skip = torch.zeros(attn_weights.size()).to(attn_weights.get_device())\n",
    "        # joint_attns[0] = aug_att_mat[0]\n",
    "        # joint_attns_skip[0] = attn_weights[0]\n",
    "        # for n in range(1, aug_att_mat.size()[0]):\n",
    "        #     joint_attns[n] = torch.matmul(aug_att_mat[n], joint_attns[n-1])\n",
    "        #     joint_attns_skip[n] = torch.matmul(attn_weights[n], joint_attns_skip[n-1])\n",
    "\n",
    "        feature_map = x_patch.detach().clone()    # B * C * 14 * 14\n",
    "        n, c, h, w = feature_map.shape\n",
    "        #cams = joint_attns[-1][:, 0, 1:].reshape([n, h, w]).unsqueeze(1)       # B * 1 * 14 * 14\n",
    "        cams = attn_weights.sum(0)[:, 0, 1:].reshape([n, h, w]).unsqueeze(1)\n",
    "        cams = rearrange(cams, 'n 1 h w -> n h w')\n",
    "        cams = norm_cam(cams)\n",
    "        cams = rearrange(cams, 'n h w -> n (h w)')\n",
    "        pred_cam = rearrange(cams, 'B (H W) -> B 1 H W', H=h)    \n",
    "        \n",
    "        for i, layer in enumerate(self.layers):\n",
    "            x_patch, cams = layer(x_patch, cams)\n",
    "            \n",
    "        x_logits = self.pooling(x_patch).flatten(1) + conv_cls\n",
    "        if self.training:\n",
    "            return x_logits\n",
    "        else:\n",
    "            return x_logits, pred_box*pred_cam\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self,\n",
    "                 thred=0.5,\n",
    "                 residual=True,\n",
    "                 fusion_cfg=dict(loss_rate=1, grid_size=(14, 14), iteration=4),\n",
    "                 ) -> None:\n",
    "        super().__init__()\n",
    "        self.fuse = Fuse(**fusion_cfg)\n",
    "        self.shrink = nn.Tanhshrink()\n",
    "        self.thred = nn.Parameter(torch.ones([1])*thred)\n",
    "        self.residual = residual\n",
    "\n",
    "    def forward(self, x, cam):\n",
    "        \"\"\"foward function given x and spt\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): patch tokens, tensor of shape [B D H W]\n",
    "            cam (torch.Tensor): cam values, tensor of shape [B N]\n",
    "        Returns:\n",
    "            x (torch.Tensor): patch tokens, tensor of shape [B D H W]\n",
    "            cam (torch.Tensor): cam values, tensor of shape [B N]\n",
    "        \"\"\"\n",
    "        sim = embeddings_to_cosine_similarity_matrix(rearrange(x, 'B D H W -> B (H W) D'))\n",
    "        thred = self.thred.to(x.device)\n",
    "        out_cam = einsum('b h w, b w -> b h', self.fuse(sim), cam)\n",
    "        thred = thred * cam.max(1, keepdim=True)[0]\n",
    "        out_cam = self.shrink(out_cam/thred)\n",
    "        out_cam = norm_cam(out_cam)\n",
    "        out_x = einsum('b d h w, b h w -> b d h w', x, rearrange(out_cam, 'B (H W) -> B H W', H=x.shape[-2]))\n",
    "        if self.residual:\n",
    "            x = x + out_x\n",
    "            cam = cam + out_cam\n",
    "        return x, cam\n",
    "    \n",
    "def norm_cam(cam):\n",
    "    # cam [B N]\n",
    "    if len(cam.shape) == 3:\n",
    "        cam = cam - repeat(rearrange(cam, 'B H W -> B (H W)').min(1, keepdim=True)[0], 'B 1 -> B 1 1')\n",
    "        cam = cam / repeat(rearrange(cam, 'B H W -> B (H W)').max(1, keepdim=True)[0], 'B 1 -> B 1 1')\n",
    "    elif len(cam.shape) == 2:\n",
    "        cam = cam - cam.min(1, keepdim=True)[0]\n",
    "        cam = cam / cam.max(1, keepdim=True)[0]       \n",
    "        \n",
    "    return cam\n",
    "\n",
    "def embeddings_to_cosine_similarity_matrix(tokens) :\n",
    "    \"\"\"\n",
    "    Shapes for inputs:\n",
    "    - tokens : :math:`(B, N, D)` where B is the batch size, N is the target `spatial` sequence length, D is the token representation length.\n",
    "    \n",
    "    Shapes for outputs:\n",
    "    \n",
    "    Converts a a tensor of D embeddings to an (N, N) tensor of similarities.\n",
    "    \"\"\"\n",
    "    dot = torch.einsum('bij, bkj -> bik', [tokens, tokens])\n",
    "    norm = torch.norm(tokens, p=2, dim=-1) \n",
    "    x = torch.div(dot, torch.einsum('bi, bj -> bij', norm, norm))\n",
    "    \n",
    "    return x\n",
    "# @register_model\n",
    "# def Conformer_small_patch16(pretrained=False, **kwargs):\n",
    "#     model = Conformer(patch_size=16, channel_ratio=4, embed_dim=384, depth=12, num_heads=6,\n",
    "#                       mlp_ratio=4, qkv_bias=True, **kwargs)\n",
    "#     if pretrained:\n",
    "#         raise NotImplementedError\n",
    "#     return model\n",
    "\n",
    "@register_model\n",
    "def conformer_scm_small_patch16(pretrained=False, **kwargs):\n",
    "    model = Conformer(patch_size=16, channel_ratio=4, embed_dim=384, depth=12, num_heads=6,\n",
    "                      mlp_ratio=4, qkv_bias=True, **kwargs)\n",
    "\n",
    "    if pretrained:\n",
    "        # checkpoint = torch.hub.load_state_dict_from_url(\n",
    "        #     url=\"https://download.openmmlab.com/mmclassification/v0/conformer/conformer-small-p16_3rdparty_8xb128_in1k_20211206-3065dcf5.pth\",\n",
    "        #     map_location=\"cpu\", check_hash=True\n",
    "        # )['model']\n",
    "        # https://github.com/pengzhiliang/Conformer\n",
    "        checkpoint = torch.load('pretrained/Conformer_small_patch16.pth', map_location='cpu')\n",
    "        if 'model' in checkpoint.keys():\n",
    "            checkpoint = checkpoint['model']\n",
    "        else:\n",
    "            checkpoint = checkpoint\n",
    "        model_dict = model.state_dict()\n",
    "        for k in ['trans_cls_head.weight', 'trans_cls_head.bias']:\n",
    "            if k in checkpoint and checkpoint[k].shape != model_dict[k].shape:\n",
    "                print(f\"Removing key {k} from pretrained checkpoint\")\n",
    "                del checkpoint[k]\n",
    "        for k in ['conv_cls_head.weight', 'conv_cls_head.bias']:\n",
    "            if k in checkpoint and checkpoint[k].shape != model_dict[k].shape:\n",
    "                print(f\"Removing key {k} from pretrained checkpoint\")\n",
    "                del checkpoint[k]\n",
    "        pretrained_dict = {k: v for k, v in checkpoint.items() if k in model_dict}\n",
    "        model_dict.update(pretrained_dict)\n",
    "        model.load_state_dict(model_dict)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SCM",
   "language": "python",
   "name": "scm"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

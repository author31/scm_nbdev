{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CUBDataset\n",
    "\n",
    "> Adapted from SCM/lib/datasets/cub.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp datasets.cub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from PIL import Image\n",
    "from pyparsing import original_text_for\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_transforms(cfg):\n",
    "    \n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.Resize((cfg.DATA.RESIZE_SIZE, cfg.DATA.RESIZE_SIZE)),\n",
    "        transforms.RandomCrop((cfg.DATA.CROP_SIZE, cfg.DATA.CROP_SIZE)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "         transforms.Normalize(list(map(float, cfg.DATA.IMAGE_MEAN)), list(map(float, cfg.DATA.IMAGE_STD)))\n",
    "    ])\n",
    "    test_transform = transforms.Compose([\n",
    "        transforms.Resize((cfg.DATA.CROP_SIZE, cfg.DATA.CROP_SIZE)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(list(map(float, cfg.DATA.IMAGE_MEAN)), list(map(float, cfg.DATA.IMAGE_STD)))\n",
    "    ])\n",
    "\n",
    "    orig_transform = transforms.Compose([\n",
    "        transforms.Resize((cfg.DATA.CROP_SIZE, cfg.DATA.CROP_SIZE)),\n",
    "        transforms.ToTensor(),\n",
    "        # transforms.Normalize(list(map(float, cfg.DATA.IMAGE_MEAN)), list(map(float, cfg.DATA.IMAGE_STD)))\n",
    "    ])\n",
    "    \n",
    "    test_tencrops_transform = transforms.Compose([\n",
    "        transforms.Resize((cfg.DATA.RESIZE_SIZE, cfg.DATA.RESIZE_SIZE)),\n",
    "        transforms.TenCrop(cfg.DATA.CROP_SIZE),\n",
    "        transforms.Lambda(lambda crops: torch.stack(\n",
    "                [transforms.Normalize(cfg.DATA.IMAGE_MEAN, cfg.DATA.IMAGE_STD)\n",
    "                 (transforms.ToTensor()(crop)) for crop in crops])),\n",
    "    ])\n",
    "    return train_transform, test_transform, test_tencrops_transform, orig_transform\n",
    "\n",
    "\n",
    "class CUBDataset(Dataset):\n",
    "    \"\"\" 'CUB <http://www.vision.caltech.edu/visipedia/CUB-200.html>'\n",
    "\n",
    "    Args:\n",
    "        root (string): Root directory of dataset where directory \"CUB_200_2011\" exists.\n",
    "        cfg (dict): Hyperparameter configuration.\n",
    "        is_train (bool): If True. create dataset from training set, otherwise creates from test set.\n",
    "        val (bool): validation dataset for finetuning hyperparameters.\n",
    "    \"\"\"\n",
    "    def __init__(self, root, cfg, is_train, val=False):\n",
    "\n",
    "        self.root = root\n",
    "        self.cfg = cfg\n",
    "        self.is_train = is_train\n",
    "        self.resize_size = cfg.DATA.RESIZE_SIZE\n",
    "        self.crop_size = cfg.DATA.CROP_SIZE\n",
    "\n",
    "        with open(os.path.join(root, 'images.txt'), 'r') as o:\n",
    "            self.image_list = self.remove_1st_column(o.readlines())\n",
    "        with open(os.path.join(root, 'image_class_labels.txt'), 'r') as o:\n",
    "            self.label_list = self.remove_1st_column(o.readlines())\n",
    "        with open(os.path.join(root, 'train_test_split.txt'), 'r') as o:\n",
    "            self.split_list = self.remove_1st_column(o.readlines())\n",
    "        with open(os.path.join(root, 'bounding_boxes.txt'), 'r') as o:\n",
    "            self.bbox_list = self.remove_1st_column(o.readlines())\n",
    "            \n",
    "        self.train_transform, self.onecrop_transform, self.tencrops_transform, self.orig_transform = get_transforms(cfg)\n",
    "        if cfg.TEST.TEN_CROPS:\n",
    "            self.test_transform = self.tencrops_transform\n",
    "        else:\n",
    "            self.test_transform = self.onecrop_transform\n",
    "\n",
    "        if is_train:\n",
    "            self.index_list = self.get_index(self.split_list, '1')\n",
    "        else:\n",
    "            self.index_list = self.get_index(self.split_list, '0')\n",
    "        \n",
    "        self.val = val\n",
    "        if val:\n",
    "            self.image_dir = os.path.join(self.root, 'CUBV2')\n",
    "            # val2/1/1.jpeg,1\n",
    "            datalist = os.path.join(self.root, 'CUBV2', 'val', 'image_ids.txt')\n",
    "            labelList = os.path.join(self.root, 'CUBV2', 'val', 'class_labels.txt')\n",
    "            bboxlist = os.path.join(self.root, 'CUBV2', 'val', 'localization.txt')\n",
    "            class_labels = {}\n",
    "            boxes = {}\n",
    "            dataList = []\n",
    "            with open(datalist) as f:\n",
    "                    for line in f.readlines():\n",
    "                        dataList.append(line.strip('\\n'))            \n",
    "            with open(labelList) as f:\n",
    "                for line in f.readlines():\n",
    "                    image_id, class_label = line.strip('\\n').split(',')\n",
    "                    class_labels[image_id] = int(class_label)\n",
    "            with open(bboxlist) as f:\n",
    "                for line in f.readlines():\n",
    "                    image_id, x0s, x1s, y0s, y1s = line.strip('\\n').split(',')\n",
    "                    x0, x1, y0, y1 = int(x0s), int(x1s), int(y0s), int(y1s)\n",
    "                    if image_id in boxes:\n",
    "                        boxes[image_id].append([x0, x1, y0, y1])\n",
    "                    else:\n",
    "                        boxes[image_id] = [[x0, x1, y0, y1]]                    \n",
    "            \n",
    "            self.val2_class_labels = class_labels\n",
    "            self.val2_boxes = boxes\n",
    "            self.val2_names = dataList\n",
    "\n",
    "    def get_index(self, list, value):\n",
    "        index = []\n",
    "        for i in range(len(list)):\n",
    "            if list[i] == value:\n",
    "                index.append(i)\n",
    "        return index\n",
    "\n",
    "    def remove_1st_column(self, input_list):\n",
    "        output_list = []\n",
    "        for i in range(len(input_list)):\n",
    "            if len(input_list[i][:-1].split(' '))==2:\n",
    "                output_list.append(input_list[i][:-1].split(' ')[1])\n",
    "            else:\n",
    "                output_list.append(input_list[i][:-1].split(' ')[1:])\n",
    "        return output_list\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        name = self.image_list[self.index_list[idx]]\n",
    "        image_path = os.path.join(self.root, 'images', name)\n",
    "        \n",
    "        label = int(self.label_list[self.index_list[idx]])-1\n",
    "        \n",
    "        if self.val:\n",
    "            name = self.val2_names[idx]\n",
    "            label = self.val2_class_labels[name]\n",
    "            image = Image.open(os.path.join(self.image_dir, name)).convert('RGB')\n",
    "            bbox = self.val2_boxes[name][0] # only one is available\n",
    "        else:           \n",
    "            image = Image.open(image_path).convert('RGB')\n",
    "            bbox = self.bbox_list[self.index_list[idx]]\n",
    "            bbox = [int(float(value)) for value in bbox]\n",
    "            \n",
    "        image_size = list(image.size)    \n",
    "        \n",
    "        if self.is_train:\n",
    "            image = self.train_transform(image)\n",
    "            return image, label\n",
    "        else:\n",
    "            orig = self.orig_transform(image)\n",
    "            image = self.test_transform(image)\n",
    "\n",
    "            [x, y, bbox_width, bbox_height] = bbox\n",
    "            # if self.is_train:\n",
    "            #     resize_size = self.resize_size\n",
    "            #     crop_size = self.crop_size\n",
    "            #     shift_size = (resize_size - crop_size) // 2\n",
    "            resize_size = self.crop_size\n",
    "            crop_size = self.crop_size\n",
    "            shift_size = 0\n",
    "            [image_width, image_height] = image_size\n",
    "            left_bottom_x = int(max(x / image_width * resize_size - shift_size, 0))\n",
    "            left_bottom_y = int(max(y / image_height * resize_size - shift_size, 0))\n",
    "            right_top_x = int(min((x + bbox_width) / image_width * resize_size - shift_size, crop_size - 1))\n",
    "            right_top_y = int(min((y + bbox_height) / image_height * resize_size - shift_size, crop_size - 1))\n",
    "\n",
    "            # gt_bbox = [left_bottom_x, left_bottom_y, right_top_x, right_top_y]\n",
    "            # gt_bbox = torch.tensor(gt_bbox)\n",
    "            gt_bbox = np.array([left_bottom_x, left_bottom_y, right_top_x, right_top_y]).reshape(-1)\n",
    "            gt_bbox = \" \".join(list(map(str, gt_bbox)))\n",
    "            \n",
    "            return image, label, gt_bbox, name, orig\n",
    "\n",
    "    def __len__(self):\n",
    "        ds_len= self.index_list\n",
    "        if self.val: ds_len= self.val2_names\n",
    "        return ds_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SCM",
   "language": "python",
   "name": "scm"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
